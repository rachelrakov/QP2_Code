{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get timeseries of F0s\n",
    "- Measures every 10 ms\n",
    "- Needs to be extended to multiple wav files\n",
    "- Need to add filename somehow as a feature, to keep track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from Tkinter import *\n",
    "import tkSnack\n",
    "root = Tk()\n",
    "tkSnack.initializeSnack(root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##from pylab import *\n",
    "def getSound(fname_list):\n",
    "    sound_list = []\n",
    "    for item in fname_list:\n",
    "        utterance = tkSnack.Sound()\n",
    "        utterance.read(item)\n",
    "        sound_list.append(utterance)\n",
    "    return sound_list\n",
    "\n",
    "\n",
    "path = \"c:/Python27/qp2_py/small_for_test/\"\n",
    "allFiles = os.listdir(path)\n",
    "allTxtgridsCOMB = [item for item in allFiles if item[0]== \"C\"]\n",
    "allWavs = [item for item in allFiles if item[0] !=\"C\"]\n",
    "\n",
    "allPaths = []\n",
    "for item in allWavs:\n",
    "    total_path = path+item\n",
    "    allPaths.append(total_path)\n",
    "\n",
    "allTxtgrids = []\n",
    "allTxtPaths = []\n",
    "for item in allTxtgridsCOMB:\n",
    "    outlist = item.split('COMBINE_')\n",
    "    name = outlist[1]\n",
    "    allTxtgrids.append(name)\n",
    "    allTxtPaths.append(path+name)\n",
    "\n",
    "    \n",
    "allTxtgrids.sort()\n",
    "allWavs.sort()\n",
    "\n",
    "\n",
    "\n",
    "#fname = [non_fname, native_fname]\n",
    "data_list = getSound(allPaths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To Do:\n",
    "- In this cell, get all of the name of files for baseline / test v train split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "### item.pitch returns a list of [f0 measure, something binary, intensity measure???, something else]\n",
    "## only relevant thing is the F0 measure - maybe just save that?\n",
    "def getPitch(sound_list):\n",
    "    pitch_list = []\n",
    "    for item in sound_list:\n",
    "        item.convert(channels=1)\n",
    "        pitch_tuple = item.pitch(method=\"ESPS\", windowlength = 0.010)\n",
    "        pitch_list.append(pitch_tuple)\n",
    "    return pitch_list\n",
    "\n",
    "pitch_list = getPitch(data_list)\n",
    "print len(pitch_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for item in pitch_list:\n",
    "    print item[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "286482\n",
      "286482\n",
      "28647\n",
      "28647\n"
     ]
    }
   ],
   "source": [
    "### Make a list of lists of F0 points\n",
    "\n",
    "all_f0s_list = []\n",
    "for item in pitch_list:\n",
    "    just_f0_list = [j[0] for j in item]\n",
    "    all_f0s_list.append(just_f0_list)\n",
    "\n",
    "\n",
    "    \n",
    "print len(all_f0s_list)\n",
    "for i in all_f0s_list:\n",
    "    print len(i)\n",
    "\n",
    "## output is [[series of f0s for one file][series of f0s for next file]] etc \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make TextGrid Objects\n",
    "- Also needs to be extended to multiple files\n",
    "- Edit this if you want to look at tiers other than IPU tiers (like Question Tiers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:/Python27/qp2_py/small_for_test/COMBINE_p204p205-part1_ch1.textgrid\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import tgt\n",
    "#non_txtgrd = \"c:/Python27/qp2_py/txtgrids/COMBINE_p452p453-part2_ch1.textgrid\"\n",
    "#native_txtgrd = \"c:/Python27/qp2_py/txtgrids/p226p227-part2_ch2.textgrid\"\n",
    "#all_txtgrids = [non_txtgrd, native_txtgrd]\n",
    "\n",
    "allTxtPaths = []\n",
    "path = \"c:/Python27/qp2_py/small_for_test/\"\n",
    "for item in allTxtgridsCOMB:\n",
    "    allTxtPaths.append(path+item)\n",
    "\n",
    "\n",
    "checkGrid = allTxtPaths[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'BF', u'WORD', u'LONG_WORD', u'LONG_IPUs', u'IPU_WORDs', u'LAUGHTER']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_obj= tgt.read_textgrid(checkGrid)\n",
    "grid_obj.get_tier_names()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine TextGrids with F0s\n",
    "- Needs to be extended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1804\n",
      "1549\n",
      "153\n",
      "144\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "#annotations are a list of interval objects\n",
    "inter = []\n",
    "segmented_f0s = []\n",
    "\n",
    "for i in range(len(allTxtPaths)):\n",
    "    grid_obj = tgt.read_textgrid(allTxtPaths[i])\n",
    "    word = grid_obj.get_tier_by_name('IPU_WORDs')\n",
    "    ipus = word.annotations\n",
    "    print len(ipus)\n",
    "    length = len(ipus)\n",
    "    for j in range(length):\n",
    "        start = ipus[j].start_time\n",
    "        end = ipus[j].end_time\n",
    "        word = ipus[j].text\n",
    "        #print start, end, word\n",
    "        #multiply to get the correct index\n",
    "        idx_start = int(start*100)\n",
    "        idx_end = int(end*100)\n",
    "        #print idx_start, idx_end\n",
    "        f0s = all_f0s_list[i][idx_start:idx_end]\n",
    "        entry = (start, end, f0s, word)\n",
    "        inter.append(entry)\n",
    "    segmented_f0s.append(inter)\n",
    "    inter = []\n",
    "\n",
    "    \n",
    "    \n",
    "print len(segmented_f0s)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## make this a loop\n",
    "#first_word = word.annotations[0]\n",
    "#print first_word.start_time\n",
    "#print first_word.end_time\n",
    "#print first_word.text\n",
    "\n",
    "#returns a list of lists --> inner list is a tuple of (starttime, endtime, [listofF0series],\"textstring\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = [0, 2.1, 3.2, 12.2, 0, 0]\n",
    "import math\n",
    "\n",
    "log = [math.log(item) for item in x if item !=0]\n",
    "\n",
    "print log\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "1804\n",
      "1549\n",
      "153\n",
      "144\n"
     ]
    }
   ],
   "source": [
    "###NOTES ABOUT THIS FUNCTION: \n",
    "## Extended for large data lists (2/16/2016)\n",
    "## log normalizes each F0 point in the IPU, then calculates min, max, and std of the normalized F0s\n",
    "## also calculates mean and the z-normalized mean of the logF0 points per IPU\n",
    "## currently (2/15/2016) returns a list of (ipuString, [seconds, minF0, maxF0, std, meanF0, znorm_meanF0, whisper])\n",
    "## Goal is to output format that's good for sklearn\n",
    "## NOTE:  For whispered ipus, all values are \"NaN\"\n",
    "## NOTE: For one F0 point ipus, min, max, and mean F0 features are log of the F0 point, std&znorm are NaN, whisper = 0\n",
    "\n",
    "\n",
    "## QUESTIONS FOR ANDREW (2/3/2016):\n",
    "### z-normalized is returning between 1 and -1, is that okay?\n",
    "## Is the normalization I'm doing okay\n",
    "## can I z-score normalize already log normalized f0 means?\n",
    "## QUESTIONS 2/15/2016\n",
    "## is my zscore normalization correct?\n",
    "## handling whisperes and f0 points okay?\n",
    "\n",
    "\n",
    "\n",
    "#### segmented_f0s\n",
    "\n",
    "#segmented_f0s is a list [start_time, end_time, [series of F0 measures every 10ms], \"ipu_string\"]\n",
    "import scipy\n",
    "import math\n",
    "from scipy import stats\n",
    "\n",
    "feature_set = []\n",
    "intermediate = []\n",
    "\n",
    "for i in range(len(segmented_f0s)):\n",
    "    current = segmented_f0s[i]\n",
    "    for item in current:\n",
    "    ## assign text, calculate length    \n",
    "        txt = str(item[3])\n",
    "        start_time = item[0]\n",
    "        end_time = item[1]\n",
    "        time = end_time-start_time\n",
    "\n",
    "        f0s = item[2]\n",
    "        ## log of all valid f0 points\n",
    "        logF0s = [math.log(f0) for f0 in f0s if f0 !=0]\n",
    "        ##### if no F0 points\n",
    "        if bool(logF0s) == False:\n",
    "            minF0 = \"NaN\"\n",
    "            maxF0 = \"NaN\"\n",
    "            std = 0\n",
    "            meanF0 = 0\n",
    "            znorm_mean = 0\n",
    "            whisper = 1\n",
    "        ## if only 1 F0 value    \n",
    "        elif len(logF0s) == 1:\n",
    "            oneVal = math.log(logF0s[0])\n",
    "            minF0 = oneVal\n",
    "            maxF0 = oneVal\n",
    "            std = 0\n",
    "            meanF0 = oneVal\n",
    "            znorm_mean = 0\n",
    "            whipser = 0\n",
    "        \n",
    "        else:\n",
    "            minF0 = min(logF0s)\n",
    "            maxF0 = max(logF0s)\n",
    "            meanF0 = sum(logF0s)/len(logF0s)\n",
    "            std = scipy.stats.tstd(logF0s)\n",
    "            for point in logF0s:\n",
    "                znorm_mean = (point-meanF0)/std\n",
    "            whisper = 0\n",
    "        #print \"min:%f, max:%f, std:%f, mean:%f, znormMean:%f\" %(minF0, maxF0, std, meanF0, znorm_mean)\n",
    "\n",
    "        features = [time, minF0, maxF0, std, meanF0, znorm_mean, whisper]\n",
    "        set_tup = (txt, features)\n",
    "        intermediate.append(set_tup)\n",
    "    feature_set.append(intermediate)\n",
    "    intermediate = []\n",
    "\n",
    "print len(feature_set)\n",
    "for i in range(len(feature_set)):\n",
    "    print len(feature_set[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get rid of silence & unicode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "851\n"
     ]
    }
   ],
   "source": [
    "done_features = []\n",
    "inter = []\n",
    "for ipu in feature_set:\n",
    "    for features in ipu:\n",
    "        if features[0] == \"silent\":\n",
    "            pass\n",
    "        else:\n",
    "            inter.append(features)\n",
    "    done_features.append(inter)\n",
    "    inter = []\n",
    "\n",
    "print len(done_features)\n",
    "print len(done_features[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add Pronouncing Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "851\n",
      "722\n",
      "63\n",
      "59\n"
     ]
    }
   ],
   "source": [
    "####Calculates syllables for each IPU\n",
    "## NOTE: Assumes words not in the dictionary are all monosyllabic (assumption based on um, mm, and false starts)\n",
    "### But this is definitely not accurate, there will be names that are wrong too.\n",
    "## Extended for large data (2/16/2016)\n",
    "\n",
    "import pronouncing\n",
    "syl_list = []\n",
    "intermediate = []\n",
    "\n",
    "def getWeirdSyllables(text):\n",
    "    \"\"\"Gets syllables count for IPUs that contain OOVs.  NOTE:  OOVs always return monosyllabic\"\"\"\n",
    "    syl_sum = 0\n",
    "    checkit = text.split()\n",
    "    for word in checkit:\n",
    "        valid = pronouncing.phones_for_word(word)\n",
    "        #print valid\n",
    "        if valid:\n",
    "            syl = sum([pronouncing.syllable_count(p)for p in valid[0]])\n",
    "        else:\n",
    "            syl = 1\n",
    "        syl_sum = syl_sum+syl\n",
    "    return syl_sum\n",
    "\n",
    "    \n",
    "\n",
    "for features in done_features:    \n",
    "    for item in features:\n",
    "        text = item[0]\n",
    "        text = text.lower()\n",
    "        try:\n",
    "            phones = [pronouncing.phones_for_word(p)[0] for p in text.split()]\n",
    "            syl = sum([pronouncing.syllable_count(p) for p in phones])\n",
    "\n",
    "        except IndexError:\n",
    "            syl = getWeirdSyllables(text)\n",
    "        intermediate.append(syl)\n",
    "    syl_list.append(intermediate)\n",
    "    intermediate = []\n",
    "\n",
    "\n",
    "\n",
    "print len(syl_list)\n",
    "for i in range(len(syl_list)):\n",
    "    print len(syl_list[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print syl_list[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('I', [2.85714285714286, 4.887565519227192, 5.004256144488883, 0.024685067530547552, 4.931193989930416, 1.0653449652214659, 0])\n",
      "('I WAS BORN', [3.52941176470588, 4.775653864011802, 5.245005627651339, 0.10475101059419453, 4.991952542588709, -1.3901473063097409, 0])\n",
      "('IN THE', [2.0202020202224276, 4.04456375516899, 5.15268997030611, 0.30721779326631837, 4.393030351717931, 2.413237619826464, 0])\n",
      "('DALIAN CITY', [5.434782608695652, 4.655142643091459, 5.075647679077873, 0.084121249289424452, 4.868556338415173, -1.0923475519642227, 0])\n",
      "('OH I', [1.6393442622950787, 3.9972680808918883, 4.88269672388616, 0.17116374503885409, 4.204656855596894, 3.0058094762729661, 0])\n",
      "('I THINK ITS UH', [2.816901408450701, 4.701491304262329, 5.023318498188607, 0.057672518488937251, 4.907082998298792, -3.5648121397004653, 0])\n",
      "('TWENTY THREE YEARS', [2.739726027397259, 4.145302513930246, 5.019100239971806, 0.29930440885929765, 4.742180965183827, 0.75344612616984019, 0])\n",
      "('FINISHED MY WHEN I', [2.439429140326867, 4.1484498895842785, 5.143256250935837, 0.29118127178265935, 4.7145788011029035, 0.36265458757331931, 0])\n",
      "('WHY FINISH MY', [2.1621621621621605, 4.19774056403638, 5.029531956851461, 0.15822765358489477, 4.825323040135433, -0.71911401060315305, 0])\n",
      "('UH', [1.1363636363636376, 3.9952616598137443, 4.995450910836208, 0.3350692147647033, 4.250810985571097, 2.2154058985341494, 0])\n",
      "('HOW MANY YEARS DID YOU LIVE IN', [3.6697247706422025, 5.15776615327736, 5.639393790382804, 0.1107458055989347, 5.410295181208578, 1.6748182576247865, 0])\n",
      "('YOUR FIRST HOME', [2.4590163934426252, 5.132575259333637, 5.474533400822656, 0.10046123762348551, 5.270655628843451, 0.34039655014312309, 0])\n",
      "('STUDY', [2.7027027027027084, 3.9946714507709773, 4.12834850572655, 0.032704977805096458, 4.039127976812586, -0.086261060321674007, 0])\n",
      "('THE', [0.4926108374384234, 4.000112940274555, 5.680278946186502, 0.56750110017468536, 4.682849154436324, -0.99126330850832067, 0])\n",
      "('UNIVERSITY', [4.385964912280727, 4.047180472813456, 5.645268952469509, 0.39786736866263461, 5.429990488613313, -3.4755552345193421, 0])\n",
      "('I', [4.166666666666632, 4.025365723273063, 4.872010735660546, 0.38014443985089935, 4.409735310357989, -0.84681147810331803, 0])\n",
      "('GO AWAY', [4.411764705882355, 4.717167064205469, 5.636975105586938, 0.26472790855599687, 5.457885066284723, 0.068548892629195682, 0])\n",
      "('LEAVE HOME TO', [3.092783505154643, 4.07346586712839, 5.605003802858115, 0.66088826302159209, 4.609355009416646, 1.336993230072004, 0])\n",
      "('FIND', [1.9607843137254979, 5.420481727125163, 5.4820218752127525, 0.024649765456507083, 5.451906669040365, -0.74626182570919641, 0])\n",
      "('A', [1.6393442622950836, 5.328098951920605, 5.55888358652808, 0.049911913978721745, 5.391550079845398, -0.63233953150701849, 0])\n",
      "('HELLO', [5.7142857142857055, 4.133423400143705, 5.628379923033989, 0.66576076495550518, 5.10761615956691, 0.61036529115662508, 0])\n",
      "('EL PASO', [4.054054054054058, 4.694044279405852, 5.094054768529338, 0.066599697837280092, 4.863836465777778, -0.56317892216342846, 0])\n",
      "('THATS IN TEXAS', [3.4482758620986944, 4.724222286375279, 5.068391125791739, 0.080629576259270586, 4.8710083395195, -0.34666065308503774, 0])\n",
      "('OF THEM WAS MMM', [1.7621233385911848, 4.030458429686793, 5.2362445530533686, 0.32301378437072187, 4.734011748865683, 0.35552143509831702, 0])\n",
      "('8', [4.545454545454569, 'NaN', 'NaN', 0, 0, 0, 1])\n",
      "('YEARS YES THEM', [3.488372093023258, 3.99983059322036, 5.637696424247373, 0.75571794940544679, 4.991850777233165, -1.3126857510705769, 0])\n",
      "('NO', [8.551483421339604, 4.0246795332123835, 4.047171875089274, 0.0075504944863234794, 4.04244124305629, -2.352390280673534, 0])\n",
      "('MY MOM', [9.090909090909138, 4.077533370576615, 4.115062185928132, 0.012714979416291522, 4.093735920240224, -1.1310005176374434, 0])\n",
      "('AS', [1.5384615384615419, 4.80458871064886, 5.098330903837076, 0.05721092349813995, 4.892887464640121, -0.027637418446474323, 0])\n",
      "('WELL UHH', [0.15772870662460564, 4.027895784002439, 5.431483921738193, 0.33409911325366581, 4.723085774145412, 0.1611273656183928, 0])\n",
      "('WHERE WERE YOU BORN', [5.063291139240509, 5.468036834189937, 5.671038027051569, 0.05695587550067345, 5.569536612868048, -0.98828847055132774, 0])\n",
      "(\"WHERE'S THAT\", [2.7777777777777786, 5.368007718127859, 5.728024218661765, 0.08279906445096416, 5.453607501871299, 3.3134947107600006, 0])\n",
      "('WERE', [14.28571428571459, 'NaN', 'NaN', 0, 0, 0, 1])\n",
      "('YOUR PARENTS', [4.5454545454545565, 4.84455641310716, 4.9181412745820685, 0.031250142261997994, 4.883516358435331, -1.2467125749871886, 0])\n",
      "('BORN THERE TOO', [4.109589041095888, 5.329542210271345, 5.629620430152174, 0.069771160355463616, 5.43130312615824, 1.2028005730936528, 0])\n",
      "('HOW MANY YEARS DID YOU LIVE IN YOUR FIRST HOUSE', [5.583756345177668, 5.273992920810331, 5.6464042382884845, 0.079687494019688818, 5.481802364643402, -0.34259010313296023, 0])\n",
      "('WHAT IS', [5.882352941176473, 5.338217745104486, 5.587955913137745, 0.072131428282332807, 5.4528097877940285, -1.5886562268116031, 0])\n",
      "('YOUR MOTHERS', [7.500000000000027, 4.51521469329584, 5.292187395292529, 0.35376573593755722, 4.948196371653295, 0.55119084624961534, 0])\n",
      "('JOB', [2.3490558502540986, 4.2149405233803705, 4.25454879043021, 0.0088589523676964096, 4.227002392106275, -1.2557035962527336, 0])\n",
      "(\"WHAT'S\", [1.3513513513513478, 4.125438915325453, 4.272136732118681, 0.033074626730848593, 4.183218019713138, -1.7469314123444002, 0])\n"
     ]
    }
   ],
   "source": [
    "## replaces time with seconds per syllable\n",
    "## feature_set is now done for a baseline, for many files (3/25/2016)\n",
    "\n",
    "num_datapoint = len(done_features)\n",
    "\n",
    "for num in range(num_datapoint):\n",
    "    for i in range(len(done_features[num])):\n",
    "        #print done_features[num][i][1]\n",
    "        length = float(done_features[num][i][1][0])\n",
    "        ##feature set [each featurelist][each ipu][list of features][seconds]\n",
    "        syl_num = syl_list[num][i] #get the # of syllables\n",
    "        #print length\n",
    "        #print syl_num\n",
    "        syl_sec =  syl_num/length\n",
    "        #print \"SPACE\"\n",
    "        done_features[num][i][1][0]= syl_sec #replace with syllables per second\n",
    "        #print done_features[num][i][1][0]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Build in quick check\n",
    "for num in range(len(done_features)):\n",
    "   for i in range(10):\n",
    "        print done_features[num][i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make IPU based features speaker based features\n",
    "time, minF0, maxF0, std, meanF0, znorm_mean, whisper\n",
    "\n",
    "- number ipus per speaker\n",
    "- avg syls per ipu per speaker\n",
    "- min f0 per speaker\n",
    "- max f0 per speaker\n",
    "- mean f0 per speaker\n",
    "- mean std per speaker\n",
    "- avg znorm mean per speaker\n",
    "- number of whispered ipus\n",
    "- anything else to add?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.2963261086 3.97713976283 6.29360044998 0.22995022533 4.59474519372 -0.158377275498 28\n",
      "3.39323274953 1.70147896824 6.29918023375 0.15900915467 4.74627338837 -0.236795794991 44\n",
      "3.91085144079 3.98199901571 5.79994299343 0.144599723409 4.29147453972 -0.279060370342 8\n",
      "5.223295886 4.01563925629 6.02614464746 0.102635749102 4.60126640836 0.0325411752751 6\n"
     ]
    }
   ],
   "source": [
    "syl_per_speaker = []\n",
    "minf0_per_speaker = []\n",
    "maxF0_per_speaker = []\n",
    "meanf0_per_speaker = []\n",
    "meanStd_per_speaker = []\n",
    "meanZnorm_per_speaker = []\n",
    "whispers_per_speaker = []\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "syl_sum =0\n",
    "meanf0_sum = 0\n",
    "meanstd_sum = 0\n",
    "meanZnorm_sum = 0\n",
    "whisper_tot = 0\n",
    "\n",
    "mins = []\n",
    "maxs = []\n",
    "\n",
    "[7.692307692307751, 4.778437317477203, 5.349490509827351, 0.17253538730659007, 4.908782448256691, -0.087913055167218049, 0]\n",
    "\n",
    "\n",
    "for speaker_ipus in done_features:\n",
    "    num_ipus = len(speaker_ipus)\n",
    "    for item in speaker_ipus:\n",
    "        syls = item[1][0]\n",
    "        #print syls\n",
    "        min_check =item[1][1]\n",
    "        if min_check !=\"NaN\":\n",
    "            mins.append(min_check)\n",
    "        max_check = item[1][2]\n",
    "        if max_check != 'NaN':\n",
    "            maxs.append(max_check)\n",
    "        meanf0 = item[1][3]\n",
    "        #print meanf0\n",
    "        std = item[1][4]\n",
    "        #print std\n",
    "        znorm = item[1][5]\n",
    "        #print znorm\n",
    "        whisper = item[1][6]\n",
    "        #print whisper\n",
    "        ### calculate ######\n",
    "        syl_sum = syl_sum+syls\n",
    "        meanf0_sum = meanf0+meanf0_sum\n",
    "        meanstd_sum = std+meanstd_sum\n",
    "        meanZnorm_sum = znorm+meanZnorm_sum\n",
    "        whisper_tot = whisper + whisper_tot\n",
    "    \n",
    "    #print maxs\n",
    "    #calculate & reset\n",
    "    syl_speaker = syl_sum/num_ipus\n",
    "    minF0 = min(mins)\n",
    "    maxF0 = max(maxs)\n",
    "    meanF0_speaker = meanf0_sum / num_ipus\n",
    "    meanStd_speaker = meanstd_sum/ num_ipus\n",
    "    meanZnorm_speaker = meanZnorm_sum/num_ipus\n",
    "    \n",
    "    print syl_speaker, minF0, maxF0, meanF0_speaker, meanStd_speaker, meanZnorm_speaker, whisper_tot\n",
    "    #### put them in out feature vectors\n",
    "    syl_per_speaker.append(syl_speaker)\n",
    "    minf0_per_speaker.append(minF0)\n",
    "    maxF0_per_speaker.append(maxF0)\n",
    "    meanf0_per_speaker.append(meanF0_speaker)\n",
    "    meanStd_per_speaker.append(meanStd_speaker)\n",
    "    meanZnorm_per_speaker.append(meanZnorm_speaker)\n",
    "    whispers_per_speaker.append(whisper_tot)\n",
    "    \n",
    "    #### reset for next run###\n",
    "    syl_sum =0\n",
    "    meanf0_sum = 0\n",
    "    meanstd_sum = 0\n",
    "    meanZnorm_sum = 0\n",
    "    whisper_tot = 0\n",
    "    mins = []\n",
    "    maxs = []\n",
    "    \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.296326108595352, 3.3932327495261725, 3.910851440786691, 5.223295886002508]\n",
      "[3.9771397628278025, 1.7014789682383724, 3.9819990157088103, 4.01563925629443]\n",
      "[6.2936004499773235, 6.299180233754941, 5.799942993429458, 6.026144647462959]\n",
      "[0.22995022533016798, 0.1590091546696891, 0.14459972340881, 0.10263574910219019]\n",
      "[4.594745193716154, 4.746273388368734, 4.291474539717812, 4.60126640835742]\n",
      "[-0.1583772754981301, -0.23679579499135414, -0.27906037034186543, 0.032541175275125213]\n",
      "[28, 44, 8, 6]\n"
     ]
    }
   ],
   "source": [
    "### This is where you will make your dataframes \n",
    "import pandas as pd\n",
    "print syl_per_speaker \n",
    "print minf0_per_speaker\n",
    "print maxF0_per_speaker\n",
    "print meanf0_per_speaker\n",
    "print meanStd_per_speaker\n",
    "print meanZnorm_per_speaker\n",
    "print whispers_per_speaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Syl_Per_Speaker     minF0     maxF0    meanF0   meanStd  meanZnorm  \\\n",
      "0         3.296326  3.977140  6.293600  0.229950  4.594745  -0.158377   \n",
      "1         3.393233  1.701479  6.299180  0.159009  4.746273  -0.236796   \n",
      "2         3.910851  3.981999  5.799943  0.144600  4.291475  -0.279060   \n",
      "3         5.223296  4.015639  6.026145  0.102636  4.601266   0.032541   \n",
      "\n",
      "   Total_Whispered  \n",
      "0               28  \n",
      "1               44  \n",
      "2                8  \n",
      "3                6  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "features = pd.DataFrame(syl_per_speaker, columns=[\"Syl_Per_Speaker\"])\n",
    "features[\"minF0\"] = minf0_per_speaker\n",
    "features[\"maxF0\"]=maxF0_per_speaker\n",
    "features[\"meanF0\"] = meanf0_per_speaker\n",
    "features[\"meanStd\"] = meanStd_per_speaker\n",
    "features[\"meanZnorm\"] = meanZnorm_per_speaker\n",
    "features[\"Total_Whispered\"] = whispers_per_speaker\n",
    "\n",
    "print features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mandarin\n"
     ]
    }
   ],
   "source": [
    "label_csv = pd.read_csv(\"c:/Python27/qp2_py/labels/Participants_info_200-563.csv\")\n",
    "small_test =  label_csv[4:8]\n",
    "small_labels = small_test[\"Language\"]\n",
    "\n",
    "\n",
    "features[\"labels\"] = small_labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_train =(features[features.columns[:7]])\n",
    "X_train= x_train[:3]\n",
    "X_test = x_train[3:4]\n",
    "\n",
    "\n",
    "y_train = small_labels[:3]\n",
    "y_test = small_labels[3:4]\n",
    "\n",
    "#print X_train, y_train, X_test, y_test\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['English'], dtype=object)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import cross_validation\n",
    "from sklearn import  svm\n",
    "classifier = svm.LinearSVC()\n",
    "classifier.fit(X_train, y_train)\n",
    "classifier.score(X_test, y_test)\n",
    "classifier.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#### Creates CSV dataframe, gets all text grid and wav files names\n",
    "label_csv = pd.read_csv(\"c:/Python27/qp2_py/labels/Participants_info_200-563.csv\")\n",
    "path = \"c:/Python27/qp2_py/deception_corpus_done/\"\n",
    "allFiles = os.listdir(path)\n",
    "allTxtgridsCOMB = [item for item in allFiles if item[0]== \"C\"]\n",
    "allWavs = [item for item in allFiles if item[0] !=\"C\"]\n",
    "\n",
    "allTxtgrids = []\n",
    "for item in allTxtgridsCOMB:\n",
    "    outlist = item.split('COMBINE_')\n",
    "    name = outlist[1]\n",
    "    allTxtgrids.append(name)\n",
    "\n",
    "    \n",
    "allTxtgrids.sort()\n",
    "allWavs.sort()\n",
    "\n",
    "\n",
    "wavBaseline = []\n",
    "for item in allWavs:\n",
    "    if re.search('.*-baseline_.*', item):\n",
    "        wavBaseline.append(item)\n",
    "\n",
    "### Makes a list of tuples (IDnum, channel number they have)\n",
    "IDs = []\n",
    "for item in wavBaseline:\n",
    "    searched = re.search('p(\\d.*)-', item)\n",
    "    outnum = searched.group(1)\n",
    "    ch_searched = re.search('.*_(ch\\d).*', item)\n",
    "    channel = ch_searched.group(1)\n",
    "    entry = (outnum, channel)\n",
    "    IDs.append(entry)\n",
    "\n",
    "\n",
    "## Puts tuples (ID, channel) into the label dataframe\n",
    "id_channels = []\n",
    "for i in range(len(label_csv['PID'])):\n",
    "    ID = label_csv['PID'][i]\n",
    "    for item in IDs:\n",
    "        if int(item[0]) == ID:\n",
    "            Found = item\n",
    "            break\n",
    "        else:\n",
    "            Found = False\n",
    "    if Found:\n",
    "        id_channels.append(Found)\n",
    "    else:\n",
    "        id_channels.append(\"NA\")\n",
    "        \n",
    "label_csv[\"ID_Channels\"] =  id_channels \n",
    "print label_csv\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "listy = label_csv['ID_Channels']\n",
    "print len(listy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## 4/6 /let's try this again\n",
    "\n",
    "baseline_files = []\n",
    "part1 = []\n",
    "part2 = []\n",
    "\n",
    "missingb = []\n",
    "missing = 0\n",
    "\n",
    "for item in allWavs:\n",
    "    fname = item\n",
    "    searched = re.search('.*_(ch\\d).*', item)\n",
    "    channel = searched.group(1)\n",
    "    if re.search(\"p(\\d+)p(\\d+)-(.*)_(ch\\d)\", item):\n",
    "        nonBases = re.search(\"p(\\d+)p(\\d+)-(.*)_(ch\\d)\", item)\n",
    "        id1, id2, part, testchan = nonBases.group(1), nonBases.group(2), nonBases.group(3), nonBases.group(4)\n",
    "        #print id1, id2, part, testchan\n",
    "        baseline = False\n",
    "        trainf = True\n",
    "    \n",
    "    elif re.search('p(\\d+).*', item):\n",
    "        base = re.search('p(\\d+).*', item)\n",
    "        id1 = base.group(1)\n",
    "        baseline = True\n",
    "        trainf = False\n",
    "        #print id1\n",
    "    \n",
    "\n",
    "    if baseline == True:\n",
    "        #print baseline, fname\n",
    "        for element in label_csv[\"ID_Channels\"]:\n",
    "            if type(element)== str:\n",
    "                fname = \"NA\"\n",
    "                pass\n",
    "            elif type(element) == tuple:\n",
    "                ID = element[0]\n",
    "                chan = element[1]\n",
    "                if id1 == ID:\n",
    "                    print fname\n",
    "    else:\n",
    "        print \"FALSE\"\n",
    "\n",
    "\n",
    "            \n",
    "        #compare id1\n",
    "\n",
    "print len(baseline_files)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##print allWavs[:10]\n",
    "\n",
    "###### NONE OF THIS WORKS AS OF 3/29/16\n",
    "\n",
    "### Code that makes lists that correspond with the dataframe for baseline, part1, and part2s\n",
    "baseline = []\n",
    "part1 = []\n",
    "part2 = []\n",
    "count = 0\n",
    "\n",
    "for i in range(len(label_csv['ID_Channels'])):\n",
    "    tup = label_csv['ID_Channels'][i]\n",
    "    #print tup\n",
    "    if type(tup) == tuple:\n",
    "        ID = tup[0]\n",
    "        ch = tup[1]\n",
    "\n",
    "        for item in allWavs:\n",
    "            baseline_file = False\n",
    "            searched = re.search('.*_(ch\\d).*', item)\n",
    "            channel = searched.group(1)\n",
    "            if re.search(\"p(\\d+)p(\\d+)-(.*)_(ch\\d)\", item):\n",
    "                nonBases = re.search(\"p(\\d+)p(\\d+)-(.*)_(ch\\d)\", item)\n",
    "                id1, id2, part, testchan = nonBases.group(1), nonBases.group(2), nonBases.group(3), nonBases.group(4)\n",
    "                #print id1, id2, part, testchan\n",
    "                nonbaseFound = True\n",
    "                baseFound = False\n",
    "                #print item, baseFound, nonbaseFound\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            elif re.search('p(\\d+).*', item):\n",
    "                base = re.search('p(\\d+).*', item)\n",
    "                id1 = base.group(1)\n",
    "                baseFound = True\n",
    "                nonbaseFound = False\n",
    "                #print item, baseFound, nonbaseFound\n",
    "\n",
    "\n",
    "\n",
    "              \n",
    "            if nonbaseFound == True:\n",
    "                if ch== testchan:\n",
    "                    if ID == id1 or ID == id2:\n",
    "                        nonbaseFound = True\n",
    "                        part = part\n",
    "                        break\n",
    "                else:\n",
    "                    nonbaseFound = False\n",
    "                    break\n",
    "\n",
    "\n",
    "        #print baseFound, nonbaseFound, ID, part\n",
    "        #print \"here\"\n",
    "        print baseFound, nonbaseFound, item\n",
    "        if baseFound == False and nonbaseFound == False:\n",
    "            entry = \"NA\"\n",
    "            baseline.append(entry)\n",
    "            part1.append(entry)\n",
    "            part2.append(entry)\n",
    "            test = 1\n",
    "            print test            \n",
    "            \n",
    "        elif baseFound == False and nonbaseFound == True:\n",
    "            entry = item\n",
    "            empty = \"NA\"\n",
    "            if part[-1] == 1:\n",
    "                baseline.append(empty)\n",
    "                part1.append(entry)\n",
    "                part2.append(empty)\n",
    "                test = 3\n",
    "                print test\n",
    "            elif part[-1] == 2:\n",
    "                baseline.append(empty)\n",
    "                part1.append(empty)\n",
    "                part2.append(entry)\n",
    "                test=4\n",
    "                print test\n",
    "                        \n",
    "            \n",
    "        elif baseFound == True and nonbaseFound == False:\n",
    "            entry = item\n",
    "            empty = \"NA\"\n",
    "            baseline.append(entry)\n",
    "            part1.append(empty)\n",
    "            part2.append(empty)\n",
    "            test = 2\n",
    "            print test\n",
    "        \n",
    "\n",
    "    else:\n",
    "        baseline.append(\"NA\")\n",
    "        part1.append(\"NA\")\n",
    "        part2.append(\"NA\")\n",
    "        test = 5\n",
    "        print test\n",
    "    count = count+1\n",
    "    #print count\n",
    "    print len(baseline), len(part1)\n",
    "\n",
    "\n",
    "            \n",
    " \n",
    "\n",
    "#print len(baseline)\n",
    "#print len(part1)\n",
    "#print len(part2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "label_csv[\"baselineFiles\"] = baseline\n",
    "print label_csv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
