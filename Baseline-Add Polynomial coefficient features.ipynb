{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get timeseries of F0s\n",
    "- Measures every 10 ms\n",
    "- Needs to be extended to multiple wav files\n",
    "- Need to add filename somehow as a feature, to keep track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from Tkinter import *\n",
    "import tkSnack\n",
    "root = Tk()\n",
    "tkSnack.initializeSnack(root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##from pylab import *\n",
    "\n",
    "def getSound(fname_list):\n",
    "    pitch_list = []\n",
    "    for item in fname_list:\n",
    "        utterance = tkSnack.Sound()\n",
    "        utterance.read(item)\n",
    "        utterance.convert(channels=1)\n",
    "        pitch_tuple = utterance.pitch(method=\"ESPS\", windowlength = 0.010)\n",
    "        pitch_list.append(pitch_tuple)\n",
    "        utterance.destroy()\n",
    "    return pitch_list\n",
    "\n",
    "\n",
    "\n",
    "path = \"c:/Python27/qp2_py/small_for_test/\"\n",
    "#path = \"c:/Python27/qp2_py/data/\"\n",
    "allFiles = os.listdir(path)\n",
    "allTxtgridsCOMB = [item for item in allFiles if item[0]== \"C\"]\n",
    "allWavs = [item for item in allFiles if item[0] !=\"C\"]\n",
    "\n",
    "allPaths = []\n",
    "for item in allWavs:\n",
    "    total_path = path+item\n",
    "    allPaths.append(total_path)\n",
    "\n",
    "allTxtgrids = []\n",
    "allTxtPaths = []\n",
    "for item in allTxtgridsCOMB:\n",
    "    outlist = item.split('COMBINE_')\n",
    "    name = outlist[1]\n",
    "    allTxtgrids.append(name)\n",
    "    allTxtPaths.append(path+name)\n",
    "\n",
    "    \n",
    "allTxtgrids.sort()\n",
    "allWavs.sort()\n",
    "\n",
    "\n",
    "#fname = [non_fname, native_fname]\n",
    "pitch_list = getSound(allPaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "print len(pitch_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gets all of the F0 points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "286482\n",
      "286482\n",
      "28647\n",
      "28647\n",
      "201996\n",
      "201996\n",
      "114408\n",
      "107447\n",
      "76590\n",
      "76590\n"
     ]
    }
   ],
   "source": [
    "### Make a list of lists of F0 points\n",
    "\n",
    "all_f0s_list = []\n",
    "for item in pitch_list:\n",
    "    just_f0_list = [j[0] for j in item]\n",
    "    all_f0s_list.append(just_f0_list)\n",
    "\n",
    "\n",
    "    \n",
    "print len(all_f0s_list)\n",
    "for i in all_f0s_list:\n",
    "    print len(i)\n",
    "\n",
    "## output is [[series of f0s for one file][series of f0s for next file]] etc \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make TextGrid Objects\n",
    "- Edit THIS CODE if you want to look at tiers other than IPU tiers (like Question Tiers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "import tgt\n",
    "#non_txtgrd = \"c:/Python27/qp2_py/txtgrids/COMBINE_p452p453-part2_ch1.textgrid\"\n",
    "#native_txtgrd = \"c:/Python27/qp2_py/txtgrids/p226p227-part2_ch2.textgrid\"\n",
    "#all_txtgrids = [non_txtgrd, native_txtgrd]\n",
    "\n",
    "allTxtPaths = []\n",
    "path = \"c:/Python27/qp2_py/small_for_test/\"\n",
    "for item in allTxtgridsCOMB:\n",
    "    allTxtPaths.append(path+item)\n",
    "\n",
    "\n",
    "checkGrid = allTxtPaths[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'BF', u'WORD', u'LONG_WORD', u'LONG_IPUs', u'IPU_WORDs', u'LAUGHTER']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_obj= tgt.read_textgrid(checkGrid)\n",
    "grid_obj.get_tier_names()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combines TextGrids with F0s\n",
    "- This works now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1804\n",
      "1549\n",
      "153\n",
      "144\n",
      "1433\n",
      "1161\n",
      "831\n",
      "844\n",
      "458\n",
      "420\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "#annotations are a list of interval objects\n",
    "inter = []\n",
    "segmented_f0s = []\n",
    "\n",
    "for i in range(len(allTxtPaths)):\n",
    "    grid_obj = tgt.read_textgrid(allTxtPaths[i])\n",
    "    word = grid_obj.get_tier_by_name('IPU_WORDs')\n",
    "    ipus = word.annotations\n",
    "    print len(ipus)\n",
    "    length = len(ipus)\n",
    "    for j in range(length):\n",
    "        start = ipus[j].start_time\n",
    "        end = ipus[j].end_time\n",
    "        word = ipus[j].text\n",
    "        #print start, end, word\n",
    "        #multiply to get the correct index\n",
    "        idx_start = int(start*100)\n",
    "        idx_end = int(end*100)\n",
    "        #print idx_start, idx_end\n",
    "        f0s = all_f0s_list[i][idx_start:idx_end]\n",
    "        entry = (start, end, f0s, word)\n",
    "        inter.append(entry)\n",
    "    segmented_f0s.append(inter)\n",
    "    inter = []\n",
    "\n",
    "    \n",
    "    \n",
    "print len(segmented_f0s)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## make this a loop\n",
    "#first_word = word.annotations[0]\n",
    "#print first_word.start_time\n",
    "#print first_word.end_time\n",
    "#print first_word.text\n",
    "#returns a list of lists --> inner list is a tuple of (starttime, endtime, [listofF0series],\"textstring\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This makes a lot of the baseline features\n",
    "- log normalizes each F0 point in the IPU, then calculates min, max, and std of the normalized F0s\n",
    "- also calculates mean and the z-normalized mean of the logF0 points per IPU\n",
    "- currently (2/15/2016) returns a list of (ipuString, [seconds, minF0, maxF0, std, meanF0, znorm_meanF0, whisper])\n",
    "- Goal is to output format that's good for sklearn\n",
    "- NOTE:  For whispered ipus, all values are \"NaN\"\n",
    "- NOTE: For one F0 point ipus, min, max, and mean F0 features are log of the F0 point, std&znorm are NaN, whisper = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "1804\n",
      "1549\n",
      "153\n",
      "144\n",
      "1433\n",
      "1161\n",
      "831\n",
      "844\n",
      "458\n",
      "420\n"
     ]
    }
   ],
   "source": [
    "###NOTES ABOUT THIS FUNCTION: \n",
    "## Extended for large data lists (2/16/2016)\n",
    "## log normalizes each F0 point in the IPU, then calculates min, max, and std of the normalized F0s\n",
    "## also calculates mean and the z-normalized mean of the logF0 points per IPU\n",
    "## currently (2/15/2016) returns a list of (ipuString, [seconds, minF0, maxF0, std, meanF0, znorm_meanF0, whisper])\n",
    "## Goal is to output format that's good for sklearn\n",
    "## NOTE:  For whispered ipus, all values are \"NaN\"\n",
    "## NOTE: For one F0 point ipus, min, max, and mean F0 features are log of the F0 point, std&znorm are NaN, whisper = 0\n",
    "\n",
    "\n",
    "\n",
    "#### segmented_f0s\n",
    "\n",
    "#segmented_f0s is a list [start_time, end_time, [series of F0 measures every 10ms], \"ipu_string\"]\n",
    "import scipy\n",
    "import math\n",
    "from scipy import stats\n",
    "\n",
    "feature_set = []\n",
    "intermediate = []\n",
    "\n",
    "for i in range(len(segmented_f0s)):\n",
    "    current = segmented_f0s[i]\n",
    "    for item in current:\n",
    "    ## assign text, calculate length    \n",
    "        txt = str(item[3])\n",
    "        start_time = item[0]\n",
    "        end_time = item[1]\n",
    "        time = end_time-start_time\n",
    "\n",
    "        f0s = item[2]\n",
    "        ## log of all valid f0 points\n",
    "        logF0s = [math.log(f0) for f0 in f0s if f0 !=0]\n",
    "        ##### if no F0 points\n",
    "        if bool(logF0s) == False:\n",
    "            minF0 = \"NaN\"\n",
    "            maxF0 = \"NaN\"\n",
    "            std = 0\n",
    "            meanF0 = 0\n",
    "            znorm_mean = 0\n",
    "            whisper = 1\n",
    "        ## if only 1 F0 value    \n",
    "        elif len(logF0s) == 1:\n",
    "            oneVal = math.log(logF0s[0])\n",
    "            minF0 = oneVal\n",
    "            maxF0 = oneVal\n",
    "            std = 0\n",
    "            meanF0 = oneVal\n",
    "            znorm_mean = 0\n",
    "            whipser = 0\n",
    "        \n",
    "        else:\n",
    "            minF0 = min(logF0s)\n",
    "            maxF0 = max(logF0s)\n",
    "            meanF0 = sum(logF0s)/len(logF0s)\n",
    "            std = scipy.stats.tstd(logF0s)\n",
    "            for point in logF0s:\n",
    "                znorm_mean = (point-meanF0)/std\n",
    "            whisper = 0\n",
    "        #print \"min:%f, max:%f, std:%f, mean:%f, znormMean:%f\" %(minF0, maxF0, std, meanF0, znorm_mean)\n",
    "\n",
    "        features = [time, minF0, maxF0, std, meanF0, znorm_mean, whisper]\n",
    "        set_tup = (txt, features)\n",
    "        intermediate.append(set_tup)\n",
    "    feature_set.append(intermediate)\n",
    "    intermediate = []\n",
    "\n",
    "print len(feature_set)\n",
    "for i in range(len(feature_set)):\n",
    "    print len(feature_set[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get rid of silence & unicode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'feature_set' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-4b8526679783>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mdone_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0minter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mipu\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfeature_set\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mfeatures\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mipu\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"silent\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'feature_set' is not defined"
     ]
    }
   ],
   "source": [
    "done_features = []\n",
    "inter = []\n",
    "for ipu in feature_set:\n",
    "    for features in ipu:\n",
    "        if features[0] == \"silent\":\n",
    "            pass\n",
    "        else:\n",
    "            inter.append(features)\n",
    "    done_features.append(inter)\n",
    "    inter = []\n",
    "\n",
    "print len(done_features)\n",
    "print len(done_features[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add Pronouncing Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "####Calculates syllables for each IPU\n",
    "## NOTE: Assumes words not in the dictionary are all monosyllabic (assumption based on um, mm, and false starts)\n",
    "### But this is definitely not accurate, there will be names that are wrong too.\n",
    "## Extended for large data (2/16/2016)\n",
    "\n",
    "import pronouncing\n",
    "syl_list = []\n",
    "intermediate = []\n",
    "\n",
    "def getWeirdSyllables(text):\n",
    "    \"\"\"Gets syllables count for IPUs that contain OOVs.  NOTE:  OOVs always return monosyllabic\"\"\"\n",
    "    syl_sum = 0\n",
    "    checkit = text.split()\n",
    "    for word in checkit:\n",
    "        valid = pronouncing.phones_for_word(word)\n",
    "        #print valid\n",
    "        if valid:\n",
    "            syl = sum([pronouncing.syllable_count(p)for p in valid[0]])\n",
    "        else:\n",
    "            syl = 1\n",
    "        syl_sum = syl_sum+syl\n",
    "    return syl_sum\n",
    "\n",
    "    \n",
    "\n",
    "for features in done_features:    \n",
    "    for item in features:\n",
    "        text = item[0]\n",
    "        text = text.lower()\n",
    "        try:\n",
    "            phones = [pronouncing.phones_for_word(p)[0] for p in text.split()]\n",
    "            syl = sum([pronouncing.syllable_count(p) for p in phones])\n",
    "\n",
    "        except IndexError:\n",
    "            syl = getWeirdSyllables(text)\n",
    "        intermediate.append(syl)\n",
    "    syl_list.append(intermediate)\n",
    "    intermediate = []\n",
    "\n",
    "\n",
    "\n",
    "print len(syl_list)\n",
    "#for i in range(len(syl_list)):\n",
    "#    print len(syl_list[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replace time with syllable per second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('I', [2.85714285714286, 4.887565519227192, 5.004256144488883, 0.024685067530547552, 4.931193989930416, 1.0653449652214659, 0])\n",
      "('I WAS BORN', [3.52941176470588, 4.775653864011802, 5.245005627651339, 0.10475101059419453, 4.991952542588709, -1.3901473063097409, 0])\n",
      "('IN THE', [2.0202020202224276, 4.04456375516899, 5.15268997030611, 0.30721779326631837, 4.393030351717931, 2.413237619826464, 0])\n",
      "('DALIAN CITY', [5.434782608695652, 4.655142643091459, 5.075647679077873, 0.084121249289424452, 4.868556338415173, -1.0923475519642227, 0])\n",
      "('OH I', [1.6393442622950787, 3.9972680808918883, 4.88269672388616, 0.17116374503885409, 4.204656855596894, 3.0058094762729661, 0])\n",
      "('I THINK ITS UH', [2.816901408450701, 4.701491304262329, 5.023318498188607, 0.057672518488937251, 4.907082998298792, -3.5648121397004653, 0])\n",
      "('TWENTY THREE YEARS', [2.739726027397259, 4.145302513930246, 5.019100239971806, 0.29930440885929765, 4.742180965183827, 0.75344612616984019, 0])\n",
      "('FINISHED MY WHEN I', [2.439429140326867, 4.1484498895842785, 5.143256250935837, 0.29118127178265935, 4.7145788011029035, 0.36265458757331931, 0])\n",
      "('WHY FINISH MY', [2.1621621621621605, 4.19774056403638, 5.029531956851461, 0.15822765358489477, 4.825323040135433, -0.71911401060315305, 0])\n",
      "('UH', [1.1363636363636376, 3.9952616598137443, 4.995450910836208, 0.3350692147647033, 4.250810985571097, 2.2154058985341494, 0])\n",
      "('HOW MANY YEARS DID YOU LIVE IN', [3.6697247706422025, 5.15776615327736, 5.639393790382804, 0.1107458055989347, 5.410295181208578, 1.6748182576247865, 0])\n",
      "('YOUR FIRST HOME', [2.4590163934426252, 5.132575259333637, 5.474533400822656, 0.10046123762348551, 5.270655628843451, 0.34039655014312309, 0])\n",
      "('STUDY', [2.7027027027027084, 3.9946714507709773, 4.12834850572655, 0.032704977805096458, 4.039127976812586, -0.086261060321674007, 0])\n",
      "('THE', [0.4926108374384234, 4.000112940274555, 5.680278946186502, 0.56750110017468536, 4.682849154436324, -0.99126330850832067, 0])\n",
      "('UNIVERSITY', [4.385964912280727, 4.047180472813456, 5.645268952469509, 0.39786736866263461, 5.429990488613313, -3.4755552345193421, 0])\n",
      "('I', [4.166666666666632, 4.025365723273063, 4.872010735660546, 0.38014443985089935, 4.409735310357989, -0.84681147810331803, 0])\n",
      "('GO AWAY', [4.411764705882355, 4.717167064205469, 5.636975105586938, 0.26472790855599687, 5.457885066284723, 0.068548892629195682, 0])\n",
      "('LEAVE HOME TO', [3.092783505154643, 4.07346586712839, 5.605003802858115, 0.66088826302159209, 4.609355009416646, 1.336993230072004, 0])\n",
      "('FIND', [1.9607843137254979, 5.420481727125163, 5.4820218752127525, 0.024649765456507083, 5.451906669040365, -0.74626182570919641, 0])\n",
      "('A', [1.6393442622950836, 5.328098951920605, 5.55888358652808, 0.049911913978721745, 5.391550079845398, -0.63233953150701849, 0])\n",
      "('HELLO', [5.7142857142857055, 4.133423400143705, 5.628379923033989, 0.66576076495550518, 5.10761615956691, 0.61036529115662508, 0])\n",
      "('EL PASO', [4.054054054054058, 4.694044279405852, 5.094054768529338, 0.066599697837280092, 4.863836465777778, -0.56317892216342846, 0])\n",
      "('THATS IN TEXAS', [3.4482758620986944, 4.724222286375279, 5.068391125791739, 0.080629576259270586, 4.8710083395195, -0.34666065308503774, 0])\n",
      "('OF THEM WAS MMM', [1.7621233385911848, 4.030458429686793, 5.2362445530533686, 0.32301378437072187, 4.734011748865683, 0.35552143509831702, 0])\n",
      "('8', [4.545454545454569, 'NaN', 'NaN', 0, 0, 0, 1])\n",
      "('YEARS YES THEM', [3.488372093023258, 3.99983059322036, 5.637696424247373, 0.75571794940544679, 4.991850777233165, -1.3126857510705769, 0])\n",
      "('NO', [8.551483421339604, 4.0246795332123835, 4.047171875089274, 0.0075504944863234794, 4.04244124305629, -2.352390280673534, 0])\n",
      "('MY MOM', [9.090909090909138, 4.077533370576615, 4.115062185928132, 0.012714979416291522, 4.093735920240224, -1.1310005176374434, 0])\n",
      "('AS', [1.5384615384615419, 4.80458871064886, 5.098330903837076, 0.05721092349813995, 4.892887464640121, -0.027637418446474323, 0])\n",
      "('WELL UHH', [0.15772870662460564, 4.027895784002439, 5.431483921738193, 0.33409911325366581, 4.723085774145412, 0.1611273656183928, 0])\n",
      "('WHERE WERE YOU BORN', [5.063291139240509, 5.468036834189937, 5.671038027051569, 0.05695587550067345, 5.569536612868048, -0.98828847055132774, 0])\n",
      "(\"WHERE'S THAT\", [2.7777777777777786, 5.368007718127859, 5.728024218661765, 0.08279906445096416, 5.453607501871299, 3.3134947107600006, 0])\n",
      "('WERE', [14.28571428571459, 'NaN', 'NaN', 0, 0, 0, 1])\n",
      "('YOUR PARENTS', [4.5454545454545565, 4.84455641310716, 4.9181412745820685, 0.031250142261997994, 4.883516358435331, -1.2467125749871886, 0])\n",
      "('BORN THERE TOO', [4.109589041095888, 5.329542210271345, 5.629620430152174, 0.069771160355463616, 5.43130312615824, 1.2028005730936528, 0])\n",
      "('HOW MANY YEARS DID YOU LIVE IN YOUR FIRST HOUSE', [5.583756345177668, 5.273992920810331, 5.6464042382884845, 0.079687494019688818, 5.481802364643402, -0.34259010313296023, 0])\n",
      "('WHAT IS', [5.882352941176473, 5.338217745104486, 5.587955913137745, 0.072131428282332807, 5.4528097877940285, -1.5886562268116031, 0])\n",
      "('YOUR MOTHERS', [7.500000000000027, 4.51521469329584, 5.292187395292529, 0.35376573593755722, 4.948196371653295, 0.55119084624961534, 0])\n",
      "('JOB', [2.3490558502540986, 4.2149405233803705, 4.25454879043021, 0.0088589523676964096, 4.227002392106275, -1.2557035962527336, 0])\n",
      "(\"WHAT'S\", [1.3513513513513478, 4.125438915325453, 4.272136732118681, 0.033074626730848593, 4.183218019713138, -1.7469314123444002, 0])\n",
      "('I', [1.5873015873015899, 4.468240079938837, 4.90626464028769, 0.09112745317341453, 4.586767243596565, 0.96173915716361869, 0])\n",
      "('ATTEND', [2.352941176470589, 4.358260106806524, 4.8669565622814055, 0.094614073243128011, 4.565502869541062, 1.2542736055887995, 0])\n",
      "('SCHOOL', [2.0408163265306114, 4.323457201507862, 4.714953166721343, 0.13543688858828734, 4.553393012317215, 0.49670234304782995, 0])\n",
      "('I HAVE BEEN THERE FOR', [4.854368932038829, 4.060125814206792, 4.739674749185078, 0.13801654491089727, 4.52552637980153, 0.92685684779195476, 0])\n",
      "('SIX YEARS', [2.352941176470594, 4.387812495630376, 5.127781552979263, 0.21343851881871578, 4.58388811130985, 2.548244078339811, 0])\n",
      "('BOTH', [2.500000000000009, 4.383648472470195, 4.790218011230877, 0.11924776486792461, 4.5001331029111125, 0.16305816419909447, 0])\n",
      "('PRIMARY', [4.9999999999999885, 4.313497516012505, 4.554245122777438, 0.079878412933567997, 4.451764878363671, 1.2829529362206482, 0])\n",
      "('JUNIOR HIGH SCHOOL AND SENIOR HIGH SCHOOL', [4.8128342245989275, 4.271778693991301, 4.894078617969937, 0.11544972856596201, 4.537888429826829, 1.2458629669508106, 0])\n",
      "('I LIVE IN', [4.3478260869565135, 4.421475708409789, 4.544017527903938, 0.043761329191694079, 4.499314808687671, -0.96098454704009362, 0])\n",
      "('THE', [1.5384615384615334, 4.458815134177467, 4.6354985942085, 0.060094998959584811, 4.545224885636798, -1.2818437920882253, 0])\n",
      "('UMM', [5.882352941176473, 4.847437327425992, 5.354275931964984, 0.18099825499567054, 5.185407389833034, 0.80316892630973491, 0])\n",
      "('HAVE YOU WHAT', [3.8461538461538494, 4.392764465891138, 5.089457493129453, 0.14712870035748857, 4.758134938547562, 1.9853055530322061, 0])\n",
      "('HIGH SCHOOL DID YOU ATTEND', [4.724409448818896, 4.352357268764248, 4.979258250499827, 0.15344921004075199, 4.678938999424622, 1.9571247776084892, 0])\n",
      "('OKAY', [16.666666666666774, 4.423179110467856, 4.468534877541861, 0.018764868906753451, 4.454627111703246, -1.6758977316421122, 0])\n",
      "('SO HOW MANY', [9.523809523809526, 4.490271323285008, 4.869709395371154, 0.080361909348640181, 4.732850037877068, 1.7030376530793814, 0])\n",
      "('YEARS', [11.111111111111128, 4.792947933063722, 4.911499824198476, 0.035268363119156827, 4.866169170856327, 0.52189220758025634, 0])\n",
      "('HAVE', [0.11350737797956868, 4.004011106072086, 4.911103681984758, 0.19349123457177284, 4.4408729727726755, -0.54313208362749155, 0])\n",
      "('YOU BEEN THERE', [12.0, 'NaN', 'NaN', 0, 0, 0, 1])\n",
      "('DO', [2.8571428571428457, 4.336970254912051, 4.791806887117487, 0.12272588846595089, 4.6720717753810685, -2.2329653448996201, 0])\n",
      "('YOU LIVE', [4.0, 4.172859520322278, 4.805433298566082, 0.13740158020344381, 4.6411332363239515, -2.3985927861173759, 0])\n",
      "('UM', [1.377973957183843, 4.100329677563344, 4.776339251355301, 0.13630961389427548, 4.5620263442339475, 1.5722508559636821, 0])\n",
      "('MY MOM WORKS IN HUMAN RESOURCES', [4.306904295844322, 4.331683250531696, 4.667055716913199, 0.080621547216727332, 4.517954942653994, 1.2382906249443155, 0])\n",
      "('SILENT', [1.7210423447920202, 4.679053687062091, 4.978921772414314, 0.060194327911926832, 4.845282445956081, -2.7615352585580299, 0])\n",
      "('HA', [3.3361474752871785, 'NaN', 'NaN', 0, 0, 0, 1])\n",
      "('HA HA HA', [1.5538520906123943, 4.075559929319587, 5.059741434815534, 0.17875917191361668, 4.655945938903689, 0.10311454966312175, 0])\n",
      "('# HA', [14.285714285715677, 4.809592898929009, 4.982156089746646, 0.05451090009619923, 4.850263642906588, -0.69471371995567233, 0])\n",
      "('HA HA HA', [4.939231327048596, 4.773032624452993, 5.122467135057883, 0.096074478311887271, 4.951497050348648, -0.43646257246457615, 0])\n",
      "(\"I'LL DO MY BEST\", [3.1110086886113684, 4.375733797321716, 5.703728558814747, 0.21614266661147483, 4.556632470668771, 0.037092513733314882, 0])\n",
      "('MHM', [4.897840074178393, 4.555789477303344, 4.7847779757509, 0.061861342213845767, 4.678110717178488, -1.977345390474994, 0])\n",
      "('YEAH', [2.1128934010153197, 4.390833167730867, 5.009019711587806, 0.15738551087958672, 4.591315310372905, -1.2738284580428987, 0])\n",
      "('OH', [3.2393802733639787, 5.162781315267708, 5.399383763551693, 0.066176397916861016, 5.294531725253421, -1.6272827463300814, 0])\n",
      "('HI HOW ARE YOU', [3.603603603603604, 5.158115477557041, 5.480966488603462, 0.058962540240679262, 5.240445848670975, 0.026666358752186477, 0])\n",
      "('I AM GOOD UM', [2.3952095808383236, 4.899679750945031, 5.4289523031710685, 0.1641292896511477, 5.2398832811056, 0.98488590136514997, 0])\n",
      "('I', [3.4482758620689653, 4.901472188281676, 5.413594688484287, 0.13466679379311253, 5.316018887809139, 0.025942162230597456, 0])\n",
      "('UH', [2.272727272727275, 5.15097592995618, 5.368764553134199, 0.065217298797622406, 5.269543675991052, -1.8180413513108322, 0])\n",
      "(\"I'D LIKE TO ASK YOU A\", [4.347826086956525, 4.760625599868301, 5.5369723159106705, 0.13602907381650434, 5.3053341082028265, 0.58666353455436315, 0])\n",
      "('A FEW QUESTIONS ABOUT DO YOU HA-', [5.389294734472774, 4.962305625689014, 5.700584311781673, 0.19916999732054316, 5.312938345005483, 1.0768422725940059, 0])\n",
      "('EVER HAVE A CAT BEFORE', [4.069767441860468, 4.988427004517424, 5.677625368078326, 0.17194236114161565, 5.371103376061693, 0.90388155854603069, 0])\n",
      "('OKAY', [3.1249999999999973, 4.447521650796112, 5.529087583524926, 0.26565235745498367, 5.081681809865953, -2.0772372430148898, 0])\n",
      "('NOW WHY DO YOU', [6.451612903225796, 4.297467560884797, 5.408519452497798, 0.36971517075030103, 5.132704357491368, -1.7474836303276131, 0])\n",
      "('I was BORN IN CANADA HA HA', [7.513222787558024, 5.227828314739401, 5.486065141046676, 0.060572538679872764, 5.3245097242016355, 0.3559230221750212, 0])\n",
      "('uh', [2.5973703935269357, 5.399926125069855, 5.815212818929605, 0.085993322822698851, 5.480153954818305, -0.042073846965868182, 0])\n",
      "('The PART', [4.728785055732452, 5.232551693027298, 5.44834916281103, 0.058313879072826758, 5.370139939746452, 0.40748479535060778, 0])\n",
      "('In CANADA', [6.280247943091412, 5.351862689407446, 5.960685876526234, 0.10923477043568135, 5.466949078414494, -0.45162142853812876, 0])\n",
      "('THE PART NEAR THE GREAT LAKES', [4.589382501338667, 4.147319641816182, 5.587670786271493, 0.44189620020202125, 5.108276952785261, 0.9628305096319334, 0])\n",
      "('HA', [0.7194244604316562, 4.185193051763852, 5.689813494306246, 0.55292930893279602, 4.900156242305205, 1.3309476036269299, 0])\n",
      "('HA', [1.1494252873563204, 5.793702063762886, 6.257479203019174, 0.14372654920127151, 6.001212958286863, 0.065660567561902583, 0])\n",
      "('Three YEARS', [2.6411167512690357, 5.240425085794291, 5.628614636781362, 0.089921593900711083, 5.354847862219092, -0.059349910058360676, 0])\n",
      "('#', [5.000000000000107, 'NaN', 'NaN', 0, 0, 0, 1])\n",
      "('HA HA HA', [4.205833149463077, 1.7082207522150528, 1.7082207522150528, 0, 1.7082207522150528, 0, 0])\n",
      "('WHERE WERE you BORN', [4.7058823529411775, 5.263776135518132, 5.555565675615434, 0.071296612614355437, 5.371214862431993, 2.3846958722737983, 0])\n",
      "('UH', [2.726803434839442, 4.498392990772689, 4.672055328333656, 0.031212787327950652, 4.607725353361328, 1.6733887362754687, 0])\n",
      "('WHICH PART OF CANADA HA', [5.691056910569103, 4.617741063157298, 5.613313644959967, 0.31888576943858155, 5.370491739753497, 0.48958545628262284, 0])\n",
      "('HA', [6.249999999999995, 'NaN', 'NaN', 0, 0, 0, 1])\n",
      "('OH WHERE WERE YOU BORN', [6.493506493506497, 4.156487805739293, 5.439360425252052, 0.3521633811514176, 5.167121712796899, -1.2876755103788149, 0])\n",
      "('OH WHICH PART OF CANADA', [5.147058823529401, 4.090533164796159, 5.615666417241709, 0.47599096636125554, 5.188014710468651, 0.73561608941175138, 0])\n",
      "('UH', [1.3157894736842077, 5.2241057466327465, 6.245707591602914, 0.31602186156507039, 5.560558849421524, 1.9880508447326477, 0])\n",
      "('Uh', [2.222222222222226, 4.177291374095267, 5.474705331095376, 0.34008565081250547, 4.861632634861555, 1.5855982505762756, 0])\n",
      "('SO UM', [1.5748031496062997, 3.9804340426432985, 5.768595425602729, 0.33211751814320312, 5.202215727826215, -2.1302408486933051, 0])\n",
      "('HOW MANY YEARS DID YOU LIVE', [6.086956521739138, 5.1163631739082325, 5.462667062407215, 0.082494502205391035, 5.2807876198777794, -0.3775757164745322, 0])\n"
     ]
    }
   ],
   "source": [
    "## replaces time with seconds per syllable\n",
    "## feature_set is now done for a baseline, for many files (3/25/2016)\n",
    "\n",
    "num_datapoint = len(done_features)\n",
    "\n",
    "for num in range(num_datapoint):\n",
    "    for i in range(len(done_features[num])):\n",
    "        #print done_features[num][i][1]\n",
    "        length = float(done_features[num][i][1][0])\n",
    "        ##feature set [each featurelist][each ipu][list of features][seconds]\n",
    "        syl_num = syl_list[num][i] #get the # of syllables\n",
    "        #print length\n",
    "        #print syl_num\n",
    "        syl_sec =  syl_num/length\n",
    "        #print \"SPACE\"\n",
    "        done_features[num][i][1][0]= syl_sec #replace with syllables per second\n",
    "        #print done_features[num][i][1][0]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Build in quick check\n",
    "for num in range(len(done_features)):\n",
    "   for i in range(10):\n",
    "        print done_features[num][i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make IPU based features speaker based features\n",
    "time, minF0, maxF0, std, meanF0, znorm_mean, whisper\n",
    "\n",
    "- number ipus per speaker\n",
    "- avg syls per ipu per speaker\n",
    "- min f0 per speaker\n",
    "- max f0 per speaker\n",
    "- mean f0 per speaker\n",
    "- mean std per speaker\n",
    "- avg znorm mean per speaker\n",
    "- number of whispered ipus\n",
    "- anything else to add?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.2963261086 3.97713976283 6.29360044998 0.22995022533 4.59474519372 -0.158377275498 28\n",
      "3.39323274953 1.70147896824 6.29918023375 0.15900915467 4.74627338837 -0.236795794991 44\n",
      "3.91085144079 3.98199901571 5.79994299343 0.144599723409 4.29147453972 -0.279060370342 8\n",
      "5.223295886 4.01563925629 6.02614464746 0.102635749102 4.60126640836 0.0325411752751 6\n",
      "3.65918688813 3.97777605591 6.30460533921 0.140707263474 4.34018318985 0.464780442194 35\n",
      "4.66119532409 1.50562214023 6.30210171995 0.131211218016 4.02376802209 0.150828962091 70\n",
      "3.97161896307 3.97701008203 6.2814488837 0.10875342691 4.40416205505 -0.148042728554 6\n",
      "3.70866652099 3.98405668963 6.28470267401 0.169577346882 5.12075287648 -0.724409459654 5\n",
      "4.29516952151 1.70822075222 6.29590081327 0.226367045317 5.10673299554 0.30126852966 3\n",
      "4.23034708335 3.97818874388 6.30579078658 0.210185970921 5.06227937558 0.184371644432 6\n"
     ]
    }
   ],
   "source": [
    "syl_per_speaker = []\n",
    "minf0_per_speaker = []\n",
    "maxF0_per_speaker = []\n",
    "meanf0_per_speaker = []\n",
    "meanStd_per_speaker = []\n",
    "meanZnorm_per_speaker = []\n",
    "whispers_per_speaker = []\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "syl_sum =0\n",
    "meanf0_sum = 0\n",
    "meanstd_sum = 0\n",
    "meanZnorm_sum = 0\n",
    "whisper_tot = 0\n",
    "\n",
    "mins = []\n",
    "maxs = []\n",
    "\n",
    "[7.692307692307751, 4.778437317477203, 5.349490509827351, 0.17253538730659007, 4.908782448256691, -0.087913055167218049, 0]\n",
    "\n",
    "\n",
    "for speaker_ipus in done_features:\n",
    "    num_ipus = len(speaker_ipus)\n",
    "    for item in speaker_ipus:\n",
    "        syls = item[1][0]\n",
    "        #print syls\n",
    "        min_check =item[1][1]\n",
    "        if min_check !=\"NaN\":\n",
    "            mins.append(min_check)\n",
    "        max_check = item[1][2]\n",
    "        if max_check != 'NaN':\n",
    "            maxs.append(max_check)\n",
    "        meanf0 = item[1][3]\n",
    "        #print meanf0\n",
    "        std = item[1][4]\n",
    "        #print std\n",
    "        znorm = item[1][5]\n",
    "        #print znorm\n",
    "        whisper = item[1][6]\n",
    "        #print whisper\n",
    "        ### calculate ######\n",
    "        syl_sum = syl_sum+syls\n",
    "        meanf0_sum = meanf0+meanf0_sum\n",
    "        meanstd_sum = std+meanstd_sum\n",
    "        meanZnorm_sum = znorm+meanZnorm_sum\n",
    "        whisper_tot = whisper + whisper_tot\n",
    "    \n",
    "    #print maxs\n",
    "    #calculate & reset\n",
    "    syl_speaker = syl_sum/num_ipus\n",
    "    minF0 = min(mins)\n",
    "    maxF0 = max(maxs)\n",
    "    meanF0_speaker = meanf0_sum / num_ipus\n",
    "    meanStd_speaker = meanstd_sum/ num_ipus\n",
    "    meanZnorm_speaker = meanZnorm_sum/num_ipus\n",
    "    \n",
    "    print syl_speaker, minF0, maxF0, meanF0_speaker, meanStd_speaker, meanZnorm_speaker, whisper_tot\n",
    "    #### put them in out feature vectors\n",
    "    syl_per_speaker.append(syl_speaker)\n",
    "    minf0_per_speaker.append(minF0)\n",
    "    maxF0_per_speaker.append(maxF0)\n",
    "    meanf0_per_speaker.append(meanF0_speaker)\n",
    "    meanStd_per_speaker.append(meanStd_speaker)\n",
    "    meanZnorm_per_speaker.append(meanZnorm_speaker)\n",
    "    whispers_per_speaker.append(whisper_tot)\n",
    "    \n",
    "    #### reset for next run###\n",
    "    syl_sum =0\n",
    "    meanf0_sum = 0\n",
    "    meanstd_sum = 0\n",
    "    meanZnorm_sum = 0\n",
    "    whisper_tot = 0\n",
    "    mins = []\n",
    "    maxs = []\n",
    "    \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shifting gears: Add polynomial coefficient features\n",
    "\n",
    "### Still need to do the following:\n",
    "- Figure out how to add the coefficients as features \n",
    "    - talk to Min to figure this out?\n",
    "    - SPEAKER BASED, NOT IPU BASED (no idea how to do that)\n",
    "    \n",
    "- Cluster all coefficients (unsupervised), use number of clusteres as a per speaker feature\n",
    "\n",
    "- Use all coefficients from native / non-native speakers to train an unsupervised clusterer\n",
    "    - then for each ipu per speaker get a distance measure to find closest cluster\n",
    "    - assign ipu to cluster, use total # of each cluster as features (eg. 7 in cluseter 1, 8 in cluster 2)???\n",
    "        - but how to do that unsupervised - there will be different numbers of clusters\n",
    "    - replace ipus with distances to find a mean shortest distance to a cluster?\n",
    "        - that's overgeneralized\n",
    "    - average distance per cluster? (mean distance for ipus assigned to each cluster)\n",
    "        - use these distances as features\n",
    "        - still would need to be supervised clustering probably\n",
    "        - but distnace would be a better measure of like-ness to native / non-native clusters than just amount\n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "10\n",
      "7.10607709751\n",
      "[Legendre([ 136.27722458,    4.08392844,   -7.61437943,   15.58884328,\n",
      "        -20.12287937,   20.80164829,  -23.09872346,   21.66991469], [ 7.1060771,  7.4460771], [-1.,  1.]), Legendre([ 125.46446323,   13.96628355,  -37.10251752,   15.87151241,\n",
      "        -74.87116594,   48.24172415,  -34.57087888,  -73.81690669], [ 7.8660771,  8.7060771], [-1.,  1.]), Legendre([ 79.01349707,  28.61036204,   1.41895972,  21.95945684,\n",
      "        17.25917582,  50.44920647,  -8.60946108,  -9.58677693], [  9.3160771,  10.2960771], [-1.,  1.]), Legendre([ 76.10248949, -47.85045458, -51.58604119,  11.9752069 ,\n",
      "       -51.51567175,  71.29668026,  23.0752718 , -17.17899499], [ 10.3860771,  11.2960771], [-1.,  1.]), Legendre([ 45.93672103, -42.13606115, -26.10039096,   6.05040786,\n",
      "        19.5543902 ,  19.79572528,  -9.5637322 , -22.76089029], [ 20.39294785,  21.60294785], [-1.,  1.]), Legendre([ 104.45799587,   29.55598317,   -5.13288877,    8.63260514,\n",
      "        -20.28396926,    6.54354106,  -39.36002041,   50.5203765 ], [ 22.39294785,  23.80294785], [-1.,  1.]), Legendre([ 94.17319752,  10.99247199,   5.75701786,  59.16340141,\n",
      "       -42.03116256,  33.14297685,  -0.74036028,   8.62951616], [ 24.10294785,  25.55294785], [-1.,  1.]), Legendre([ 90.16394658,   4.64705685,  25.5537074 , -25.24491315,\n",
      "       -19.62280143, -37.62635961, -60.98436689, -14.22742613], [ 25.81294785,  27.85294785], [-1.,  1.]), Legendre([ 85.1324259 ,  21.33847034,  11.53283103, -22.14529137,\n",
      "       -73.38856461,  23.64679204, -62.50321093,  20.18380211], [ 28.70260771,  30.54260771], [-1.,  1.]), Legendre([ 28.1258038 ,  44.20160099,   4.62506692, -20.76989717,\n",
      "         9.6245316 ,  45.70625002,  40.41879225,  17.31352594], [ 31.08260771,  31.95260771], [-1.,  1.])]\n"
     ]
    }
   ],
   "source": [
    "### takes lists of IPUs per speaker, creates 7 degree Legendre polynomial coefficients for each IPU,\n",
    "### result is a matching list with an inner list of legendre transformed ipus\n",
    "\n",
    "import numpy\n",
    "from numpy.polynomial import Legendre as L\n",
    "print len(segmented_f0s)\n",
    "\n",
    "inter = []\n",
    "ipu_f0s = []\n",
    "\n",
    "for wavfile in segmented_f0s:\n",
    "    for ipu in wavfile:\n",
    "        if ipu[3] == \"silent\":\n",
    "            pass\n",
    "        else:\n",
    "            inter.append(ipu)\n",
    "    ipu_f0s.append(inter)\n",
    "    inter = []\n",
    "\n",
    "print len(ipu_f0s)\n",
    "\n",
    "degree = 7\n",
    "print ipu_f0s[0][0][0]\n",
    "\n",
    "legendre_list = []\n",
    "inter_l = []\n",
    "\n",
    "for ipu in ipu_f0s:\n",
    "    for i in range(len(ipu)):\n",
    "        start = ipu[i][0]\n",
    "        end = ipu [i][1]\n",
    "        f0s = ipu[i][2]\n",
    "        x_time = []\n",
    "        for j in range(len(f0s)):\n",
    "            x_time.append(start+(0.01*j))\n",
    "        fit = L.fit(x_time, f0s, degree)\n",
    "        inter_l.append(fit)\n",
    "    legendre_list.append(inter_l)\n",
    "    inter_l = []\n",
    "        \n",
    "        \n",
    "print legendre_list[0][:10]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "x = []\n",
    "set_check = ipu_f0s[0][0][2]\n",
    "print set_check\n",
    "\n",
    "for i in range(len(set_check)):\n",
    "    x.append(ipu_f0s[0][0][1]+(0.01*i))\n",
    "    \n",
    "print len(set_check)\n",
    "print len(x)\n",
    "\n",
    "fit = L.fit(x, set_check, degree)\n",
    "print fit\n",
    "fit.degree()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataframes get made below.  Make polynomial coefficient features above here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.296326108595352, 3.3932327495261725, 3.910851440786691, 5.223295886002508, 3.6591868881314786, 4.661195324090227, 3.9716189630721312, 3.708666520987404, 4.295169521507162, 4.230347083347124]\n",
      "[3.9771397628278025, 1.7014789682383724, 3.9819990157088103, 4.01563925629443, 3.9777760559076736, 1.5056221402338008, 3.977010082031824, 3.9840566896282157, 1.7082207522150528, 3.9781887438756485]\n",
      "[6.2936004499773235, 6.299180233754941, 5.799942993429458, 6.026144647462959, 6.30460533920825, 6.302101719953073, 6.281448883696522, 6.284702674008171, 6.295900813273147, 6.305790786577447]\n",
      "[0.22995022533016798, 0.1590091546696891, 0.14459972340881, 0.10263574910219019, 0.14070726347417614, 0.13121121801649052, 0.10875342690959565, 0.16957734688166995, 0.22636704531651272, 0.21018597092139246]\n",
      "[4.594745193716154, 4.746273388368734, 4.291474539717812, 4.60126640835742, 4.34018318985011, 4.023768022086238, 4.404162055052157, 5.120752876477532, 5.106732995543153, 5.062279375582871]\n",
      "[-0.1583772754981301, -0.23679579499135414, -0.27906037034186543, 0.032541175275125213, 0.46478044219435755, 0.15082896209081592, -0.14804272855423681, -0.72440945965437997, 0.30126852965992118, 0.18437164443247847]\n",
      "[28, 44, 8, 6, 35, 70, 6, 5, 3, 6]\n"
     ]
    }
   ],
   "source": [
    "### This is where you will make your dataframes \n",
    "import pandas as pd\n",
    "print syl_per_speaker \n",
    "print minf0_per_speaker\n",
    "print maxF0_per_speaker\n",
    "print meanf0_per_speaker\n",
    "print meanStd_per_speaker\n",
    "print meanZnorm_per_speaker\n",
    "print whispers_per_speaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Syl_Per_Speaker     minF0     maxF0    meanF0   meanStd  meanZnorm  \\\n",
      "0         3.296326  3.977140  6.293600  0.229950  4.594745  -0.158377   \n",
      "1         3.393233  1.701479  6.299180  0.159009  4.746273  -0.236796   \n",
      "2         3.910851  3.981999  5.799943  0.144600  4.291475  -0.279060   \n",
      "3         5.223296  4.015639  6.026145  0.102636  4.601266   0.032541   \n",
      "4         3.659187  3.977776  6.304605  0.140707  4.340183   0.464780   \n",
      "5         4.661195  1.505622  6.302102  0.131211  4.023768   0.150829   \n",
      "6         3.971619  3.977010  6.281449  0.108753  4.404162  -0.148043   \n",
      "7         3.708667  3.984057  6.284703  0.169577  5.120753  -0.724409   \n",
      "8         4.295170  1.708221  6.295901  0.226367  5.106733   0.301269   \n",
      "9         4.230347  3.978189  6.305791  0.210186  5.062279   0.184372   \n",
      "\n",
      "   Total_Whispered  \n",
      "0               28  \n",
      "1               44  \n",
      "2                8  \n",
      "3                6  \n",
      "4               35  \n",
      "5               70  \n",
      "6                6  \n",
      "7                5  \n",
      "8                3  \n",
      "9                6  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "features = pd.DataFrame(syl_per_speaker, columns=[\"Syl_Per_Speaker\"])\n",
    "features[\"minF0\"] = minf0_per_speaker\n",
    "features[\"maxF0\"]=maxF0_per_speaker\n",
    "features[\"meanF0\"] = meanf0_per_speaker\n",
    "features[\"meanStd\"] = meanStd_per_speaker\n",
    "features[\"meanZnorm\"] = meanZnorm_per_speaker\n",
    "features[\"Total_Whispered\"] = whispers_per_speaker\n",
    "\n",
    "print features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "########pass on this for now, do it in the next cell ##############\n",
    "\n",
    "label_csv = pd.read_csv(\"c:/Python27/qp2_py/labels/Participants_info_200-563.csv\")\n",
    "small_test =  label_csv[4:8]\n",
    "\n",
    "print small_test\n",
    "small_labels = small_test[\"Language\"]\n",
    "\n",
    "\n",
    "features[\"labels\"] = small_labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     PID  Group   Gender  Language ID_Channels\n",
      "0    200     55   Female  Mandarin          NA\n",
      "1    201     55   Female   English          NA\n",
      "2    202     56   Female   English          NA\n",
      "3    203     56   Female   English          NA\n",
      "4    204     57   Female  Mandarin  (204, ch2)\n",
      "5    205     57     Male  Mandarin  (205, ch1)\n",
      "6    206     58   Female   English  (206, ch2)\n",
      "7    207     58     Male   English  (207, ch1)\n",
      "8    208     59     Male  Mandarin  (208, ch2)\n",
      "9    209     59     Male  Mandarin  (209, ch1)\n",
      "10   210     60     Male  Mandarin  (210, ch2)\n",
      "11   211     60   Female   English  (211, ch1)\n",
      "12   212     61   Female  Mandarin  (212, ch2)\n",
      "13   213     61   Female   English  (213, ch1)\n",
      "14   214    NaN      NaN       NaN          NA\n",
      "15   215    NaN      NaN       NaN          NA\n",
      "16   216     62   Female   English  (216, ch2)\n",
      "17   217     62   Female  Mandarin  (217, ch1)\n",
      "18   218    NaN     Male       NaN          NA\n",
      "19   219    NaN      NaN       NaN          NA\n",
      "20   220     63   Female  Mandarin  (220, ch2)\n",
      "21   221     63   Female  Mandarin  (221, ch1)\n",
      "22   222     64     Male  Mandarin  (222, ch2)\n",
      "23   223     64     Male   English  (223, ch1)\n",
      "24   224     65     Male   English  (224, ch2)\n",
      "25   225     65   Female   English  (225, ch1)\n",
      "26   226     66     Male   English  (226, ch2)\n",
      "27   227     66     Male   English  (227, ch1)\n",
      "28   228     67   Female   English  (228, ch2)\n",
      "29   229     67     Male   English  (229, ch1)\n",
      "..   ...    ...      ...       ...         ...\n",
      "334  534    221     Male  Mandarin  (534, ch2)\n",
      "335  535    221   Female  Mandarin  (535, ch1)\n",
      "336  536    222     Male  Mandarin          NA\n",
      "337  537    222   Female   English          NA\n",
      "338  538    223   Female   English          NA\n",
      "339  539    223     Male  Mandarin          NA\n",
      "340  540    224   Female  Mandarin          NA\n",
      "341  541    224     Male   English          NA\n",
      "342  542    225     Male   English          NA\n",
      "343  543    225   Female  Mandarin          NA\n",
      "344  544    226     Male  Mandarin          NA\n",
      "345  545    226   Female   English          NA\n",
      "346  546    227     Male  Mandarin          NA\n",
      "347  547    227   Female   English          NA\n",
      "348  548    228   Female   English          NA\n",
      "349  549    228     Male  Mandarin          NA\n",
      "350  550    229   Female   English          NA\n",
      "351  551    229     Male  Mandarin          NA\n",
      "352  552    230   Female   English          NA\n",
      "353  553    230     Male  Mandarin          NA\n",
      "354  554    231     Male  Mandarin          NA\n",
      "355  555    231   Female   English          NA\n",
      "356  556    232     Male   Chinese          NA\n",
      "357  557    232  Female   English           NA\n",
      "358  558    233   Female   English          NA\n",
      "359  559    233     Male   Chinese          NA\n",
      "360  560    234     Male   Chinese          NA\n",
      "361  561    234   Female   English          NA\n",
      "362  562    235   Female   English          NA\n",
      "363  563    235     Male   Chinese          NA\n",
      "\n",
      "[364 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "#### Creates CSV dataframe, gets all text grid and wav files names\n",
    "label_csv = pd.read_csv(\"c:/Python27/qp2_py/labels/Participants_info_200-563.csv\")\n",
    "path = \"c:/Python27/qp2_py/wav_baseline/\"\n",
    "allFiles = os.listdir(path)\n",
    "allTxtgridsCOMB = [item for item in allFiles if item[0]== \"C\"]\n",
    "allWavs = [item for item in allFiles if item[0] !=\"C\"]\n",
    "\n",
    "allTxtgrids = []\n",
    "for item in allTxtgridsCOMB:\n",
    "    outlist = item.split('COMBINE_')\n",
    "    name = outlist[1]\n",
    "    allTxtgrids.append(name)\n",
    "\n",
    "    \n",
    "allTxtgrids.sort()\n",
    "allWavs.sort()\n",
    "\n",
    "\n",
    "wavBaseline = []\n",
    "for item in allWavs:\n",
    "    if re.search('.*-baseline_.*', item):\n",
    "        wavBaseline.append(item)\n",
    "\n",
    "### Makes a list of tuples (IDnum, channel number they have)\n",
    "IDs = []\n",
    "for item in wavBaseline:\n",
    "    searched = re.search('p(\\d.*)-', item)\n",
    "    outnum = searched.group(1)\n",
    "    ch_searched = re.search('.*_(ch\\d).*', item)\n",
    "    channel = ch_searched.group(1)\n",
    "    entry = (outnum, channel)\n",
    "    IDs.append(entry)\n",
    "\n",
    "\n",
    "## Puts tuples (ID, channel) into the label dataframe\n",
    "id_channels = []\n",
    "for i in range(len(label_csv['PID'])):\n",
    "    ID = label_csv['PID'][i]\n",
    "    for item in IDs:\n",
    "        if int(item[0]) == ID:\n",
    "            Found = item\n",
    "            break\n",
    "        else:\n",
    "            Found = False\n",
    "    if Found:\n",
    "        id_channels.append(Found)\n",
    "    else:\n",
    "        id_channels.append(\"NA\")\n",
    "        \n",
    "label_csv[\"ID_Channels\"] =  id_channels \n",
    "print label_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Mandarin', 'Mandarin', 'English', 'English', 'Mandarin', 'Mandarin', 'English', 'Mandarin', 'Mandarin', 'Mandarin']\n"
     ]
    }
   ],
   "source": [
    "data_path = \"c:/Python27/qp2_py/small_for_test/\"\n",
    "allFiles = os.listdir(data_path)\n",
    "allCurrentWavs = [item for item in allFiles if item[0] !=\"C\"]\n",
    "\n",
    "#print allCurrentWavs\n",
    "\n",
    "small_test =  label_csv[4:10]\n",
    "\n",
    "\n",
    "add1 = label_csv.loc[65]\n",
    "add2 = label_csv.loc[186]\n",
    "add3 = label_csv.loc[217]\n",
    "add4 = label_csv.loc[216]\n",
    "\n",
    "\n",
    "small_test = small_test.append(add1)\n",
    "small_test = small_test.append(add2)\n",
    "small_test = small_test.append(add3)\n",
    "small_test = small_test.append(add4)\n",
    "#print small_test\n",
    "\n",
    "label_list = []\n",
    "x = small_test[\"Language\"]\n",
    "for i in x:\n",
    "    label_list.append(i)\n",
    "\n",
    "print label_list\n",
    "features[\"labels\"] = label_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Syl_Per_Speaker     minF0     maxF0    meanF0   meanStd  meanZnorm  \\\n",
      "0         3.296326  3.977140  6.293600  0.229950  4.594745  -0.158377   \n",
      "1         3.393233  1.701479  6.299180  0.159009  4.746273  -0.236796   \n",
      "2         3.910851  3.981999  5.799943  0.144600  4.291475  -0.279060   \n",
      "3         5.223296  4.015639  6.026145  0.102636  4.601266   0.032541   \n",
      "4         3.659187  3.977776  6.304605  0.140707  4.340183   0.464780   \n",
      "5         4.661195  1.505622  6.302102  0.131211  4.023768   0.150829   \n",
      "6         3.971619  3.977010  6.281449  0.108753  4.404162  -0.148043   \n",
      "7         3.708667  3.984057  6.284703  0.169577  5.120753  -0.724409   \n",
      "8         4.295170  1.708221  6.295901  0.226367  5.106733   0.301269   \n",
      "9         4.230347  3.978189  6.305791  0.210186  5.062279   0.184372   \n",
      "\n",
      "   Total_Whispered    labels  \n",
      "0               28  Mandarin  \n",
      "1               44  Mandarin  \n",
      "2                8   English  \n",
      "3                6   English  \n",
      "4               35  Mandarin  \n",
      "5               70  Mandarin  \n",
      "6                6   English  \n",
      "7                5  Mandarin  \n",
      "8                3  Mandarin  \n",
      "9                6  Mandarin  \n"
     ]
    }
   ],
   "source": [
    "print features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_path = \"c:/Python27/qp2_py/data/\"\n",
    "allFiles = os.listdir(data_path)\n",
    "allCurrentWavs = [item for item in allFiles if item[0] !=\"C\"]\n",
    "\n",
    "### not sure what to do about this code\n",
    "for wav in allCurrentWavs:\n",
    "    searched = re.search('.*_(ch\\d).*', wav)\n",
    "    channel = searched.group(1)\n",
    "    if re.search(\"p(\\d+)p(\\d+)-(.*)_(ch\\d)\", wav):\n",
    "        nonBases = re.search(\"p(\\d+)p(\\d+)-(.*)_(ch\\d)\", wav)\n",
    "        id1, id2 = nonBases.group(1), nonBases.group(2)\n",
    "        #print id1, id2, channel\n",
    "    for label in label_csv[\"ID_Channels\"]:\n",
    "        if type(label)== tuple:\n",
    "            y = label[0]\n",
    "            if id1 == y:\n",
    "                take = label\n",
    "                break\n",
    "    #print wav, take\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "small_test =  label_csv[4:10]\n",
    "test = (label_csv.loc[1])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print small_test\n",
    "small_labels = small_test[\"Language\"]\n",
    "\n",
    "\n",
    "#features[\"labels\"] = small_labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Syl_Per_Speaker     minF0     maxF0    meanF0   meanStd  meanZnorm  \\\n",
      "0         3.296326  3.977140  6.293600  0.229950  4.594745  -0.158377   \n",
      "1         3.393233  1.701479  6.299180  0.159009  4.746273  -0.236796   \n",
      "2         3.910851  3.981999  5.799943  0.144600  4.291475  -0.279060   \n",
      "3         5.223296  4.015639  6.026145  0.102636  4.601266   0.032541   \n",
      "4         3.659187  3.977776  6.304605  0.140707  4.340183   0.464780   \n",
      "\n",
      "   Total_Whispered  \n",
      "0               28  \n",
      "1               44  \n",
      "2                8  \n",
      "3                6  \n",
      "4               35   ['Mandarin', 'Mandarin', 'English', 'English', 'Mandarin']    Syl_Per_Speaker     minF0     maxF0    meanF0   meanStd  meanZnorm  \\\n",
      "5         4.661195  1.505622  6.302102  0.131211  4.023768   0.150829   \n",
      "6         3.971619  3.977010  6.281449  0.108753  4.404162  -0.148043   \n",
      "7         3.708667  3.984057  6.284703  0.169577  5.120753  -0.724409   \n",
      "8         4.295170  1.708221  6.295901  0.226367  5.106733   0.301269   \n",
      "9         4.230347  3.978189  6.305791  0.210186  5.062279   0.184372   \n",
      "\n",
      "   Total_Whispered  \n",
      "5               70  \n",
      "6                6  \n",
      "7                5  \n",
      "8                3  \n",
      "9                6   ['Mandarin', 'English', 'Mandarin', 'Mandarin', 'Mandarin']\n"
     ]
    }
   ],
   "source": [
    "x_train =(features[features.columns[:7]])\n",
    "X_train= x_train[:5]\n",
    "X_test = x_train[5:10]\n",
    "\n",
    "\n",
    "y_train = label_list[:5]\n",
    "y_test = label_list[5:10]\n",
    "\n",
    "print X_train, y_train, X_test, y_test\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.40000000000000002"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import cross_validation\n",
    "from sklearn import  svm\n",
    "classifier = svm.LinearSVC()\n",
    "classifier.fit(X_train, y_train)\n",
    "classifier.score(X_test, y_test)\n",
    "#classifier.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "listy = label_csv['ID_Channels']\n",
    "print len(listy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## 4/6 /let's try this again\n",
    "\n",
    "baseline_files = []\n",
    "part1 = []\n",
    "part2 = []\n",
    "\n",
    "missingb = []\n",
    "missing = 0\n",
    "\n",
    "for item in allWavs:\n",
    "    fname = item\n",
    "    searched = re.search('.*_(ch\\d).*', item)\n",
    "    channel = searched.group(1)\n",
    "    if re.search(\"p(\\d+)p(\\d+)-(.*)_(ch\\d)\", item):\n",
    "        nonBases = re.search(\"p(\\d+)p(\\d+)-(.*)_(ch\\d)\", item)\n",
    "        id1, id2, part, testchan = nonBases.group(1), nonBases.group(2), nonBases.group(3), nonBases.group(4)\n",
    "        #print id1, id2, part, testchan\n",
    "        baseline = False\n",
    "        trainf = True\n",
    "    \n",
    "    elif re.search('p(\\d+).*', item):\n",
    "        base = re.search('p(\\d+).*', item)\n",
    "        id1 = base.group(1)\n",
    "        baseline = True\n",
    "        trainf = False\n",
    "        #print id1\n",
    "    \n",
    "\n",
    "    if baseline == True:\n",
    "        #print baseline, fname\n",
    "        for element in label_csv[\"ID_Channels\"]:\n",
    "            if type(element)== str:\n",
    "                fname = \"NA\"\n",
    "                pass\n",
    "            elif type(element) == tuple:\n",
    "                ID = element[0]\n",
    "                chan = element[1]\n",
    "                if id1 == ID:\n",
    "                    print fname\n",
    "    else:\n",
    "        print \"FALSE\"\n",
    "\n",
    "\n",
    "            \n",
    "        #compare id1\n",
    "\n",
    "print len(baseline_files)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##print allWavs[:10]\n",
    "\n",
    "###### NONE OF THIS WORKS AS OF 3/29/16\n",
    "\n",
    "### Code that makes lists that correspond with the dataframe for baseline, part1, and part2s\n",
    "baseline = []\n",
    "part1 = []\n",
    "part2 = []\n",
    "count = 0\n",
    "\n",
    "for i in range(len(label_csv['ID_Channels'])):\n",
    "    tup = label_csv['ID_Channels'][i]\n",
    "    #print tup\n",
    "    if type(tup) == tuple:\n",
    "        ID = tup[0]\n",
    "        ch = tup[1]\n",
    "\n",
    "        for item in allWavs:\n",
    "            baseline_file = False\n",
    "            searched = re.search('.*_(ch\\d).*', item)\n",
    "            channel = searched.group(1)\n",
    "            if re.search(\"p(\\d+)p(\\d+)-(.*)_(ch\\d)\", item):\n",
    "                nonBases = re.search(\"p(\\d+)p(\\d+)-(.*)_(ch\\d)\", item)\n",
    "                id1, id2, part, testchan = nonBases.group(1), nonBases.group(2), nonBases.group(3), nonBases.group(4)\n",
    "                #print id1, id2, part, testchan\n",
    "                nonbaseFound = True\n",
    "                baseFound = False\n",
    "                #print item, baseFound, nonbaseFound\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            elif re.search('p(\\d+).*', item):\n",
    "                base = re.search('p(\\d+).*', item)\n",
    "                id1 = base.group(1)\n",
    "                baseFound = True\n",
    "                nonbaseFound = False\n",
    "                #print item, baseFound, nonbaseFound\n",
    "\n",
    "\n",
    "\n",
    "              \n",
    "            if nonbaseFound == True:\n",
    "                if ch== testchan:\n",
    "                    if ID == id1 or ID == id2:\n",
    "                        nonbaseFound = True\n",
    "                        part = part\n",
    "                        break\n",
    "                else:\n",
    "                    nonbaseFound = False\n",
    "                    break\n",
    "\n",
    "\n",
    "        #print baseFound, nonbaseFound, ID, part\n",
    "        #print \"here\"\n",
    "        print baseFound, nonbaseFound, item\n",
    "        if baseFound == False and nonbaseFound == False:\n",
    "            entry = \"NA\"\n",
    "            baseline.append(entry)\n",
    "            part1.append(entry)\n",
    "            part2.append(entry)\n",
    "            test = 1\n",
    "            print test            \n",
    "            \n",
    "        elif baseFound == False and nonbaseFound == True:\n",
    "            entry = item\n",
    "            empty = \"NA\"\n",
    "            if part[-1] == 1:\n",
    "                baseline.append(empty)\n",
    "                part1.append(entry)\n",
    "                part2.append(empty)\n",
    "                test = 3\n",
    "                print test\n",
    "            elif part[-1] == 2:\n",
    "                baseline.append(empty)\n",
    "                part1.append(empty)\n",
    "                part2.append(entry)\n",
    "                test=4\n",
    "                print test\n",
    "                        \n",
    "            \n",
    "        elif baseFound == True and nonbaseFound == False:\n",
    "            entry = item\n",
    "            empty = \"NA\"\n",
    "            baseline.append(entry)\n",
    "            part1.append(empty)\n",
    "            part2.append(empty)\n",
    "            test = 2\n",
    "            print test\n",
    "        \n",
    "\n",
    "    else:\n",
    "        baseline.append(\"NA\")\n",
    "        part1.append(\"NA\")\n",
    "        part2.append(\"NA\")\n",
    "        test = 5\n",
    "        print test\n",
    "    count = count+1\n",
    "    #print count\n",
    "    print len(baseline), len(part1)\n",
    "\n",
    "\n",
    "            \n",
    " \n",
    "\n",
    "#print len(baseline)\n",
    "#print len(part1)\n",
    "#print len(part2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "label_csv[\"baselineFiles\"] = baseline\n",
    "print label_csv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
